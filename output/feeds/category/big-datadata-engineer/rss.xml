<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>YiLi - Big data&amp;Data Engineer</title><link>https://jelly123456.github.io/</link><description>A data scientist - A coder</description><lastBuildDate>Wed, 21 Apr 2021 00:00:00 +0800</lastBuildDate><item><title>Apache spark</title><link>https://jelly123456.github.io/2021/04/21/apache-spark/</link><description>&lt;h1 id="source"&gt;Source&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.udemy.com/course/scala-and-spark-2-getting-started/learn/lecture/9656744?LSNPUBID=JVFxdTr9V80&amp;amp;components=purchase%2Ccacheable_buy_button%2Cbuy_button%2Crecommendation&amp;amp;ranEAID=JVFxdTr9V80&amp;amp;ranMID=39197&amp;amp;ranSiteID=JVFxdTr9V80-NEYgc584Fxj.ju4fAiJ35A&amp;amp;utm_medium=udemyads&amp;amp;utm_source=aff-campaign#overview"&gt;Source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.scnsoft.com/blog/spark-vs-hadoop-mapreduce"&gt;Apache spark &lt;span class="caps"&gt;VS&lt;/span&gt;&amp;nbsp;Hadoop&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="apache-spark"&gt;Apache&amp;nbsp;spark&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;A shiny new big data&amp;nbsp;platform&lt;/li&gt;
&lt;li&gt;A open source project by Apache Software&amp;nbsp;Foundation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="apache-spark-vs-hadoop"&gt;Apache spark &lt;span class="caps"&gt;VS&lt;/span&gt;&amp;nbsp;Hadoop&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Key&amp;nbsp;difference&lt;/li&gt;
&lt;li&gt;Spark can do in&amp;nbsp;memory&lt;/li&gt;
&lt;li&gt;Hadoop mapReduce has to read from and write to a&amp;nbsp;disk&lt;/li&gt;
&lt;li&gt;the speed of processing differs&amp;nbsp;significantly&lt;/li&gt;
&lt;li&gt;Apache Spark is potentially 100 times faster than Hadoop&amp;nbsp;MapReduce&lt;/li&gt;
&lt;li&gt;the volume of data processed also&amp;nbsp;differs&lt;/li&gt;
&lt;li&gt;Hadoop MapReduce is able to work with far larger data sets than&amp;nbsp;Spark&lt;/li&gt;
&lt;li&gt;Apache Spark utilizes &lt;span class="caps"&gt;RAM&lt;/span&gt; and isn&amp;#8217;t tied to Hadoop&amp;#8217;s two-stage&amp;nbsp;paradigm&lt;/li&gt;
&lt;li&gt;Apache Spark works well for smaller data sets that can all fit into a server&amp;#8217;s &lt;span class="caps"&gt;RAM&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Hadoop is more cost effective processing massive data&amp;nbsp;sets&lt;/li&gt;
&lt;li&gt;Apache Spark is now more popular that Hadoop&amp;nbsp;MapReduce&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tasks-hadoop-mapreduce-is-good-for"&gt;Tasks Hadoop MapReduce is good&amp;nbsp;for:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linear processing of huge data&amp;nbsp;sets.&lt;/li&gt;
&lt;li&gt;Hadoop MapReduce allows parallel processing of huge amounts of data. It breaks a large chunk into smaller ones to be processed separately on different data nodes and automatically gathers the results across the multiple nodes to return a single result. In case the resulting dataset is larger than available &lt;span class="caps"&gt;RAM&lt;/span&gt;, Hadoop MapReduce may outperform&amp;nbsp;Spark.&lt;/li&gt;
&lt;li&gt;Economical solution, if no immediate results are&amp;nbsp;expected.&lt;/li&gt;
&lt;li&gt;MapReduce is a good solution if the speed of processing is not critical. For instance, if data processing can be done during night hours, it makes sense to consider using Hadoop&amp;nbsp;MapReduce.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tasks-spark-is-good-for"&gt;Tasks Spark is good&amp;nbsp;for:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Fast data&amp;nbsp;processing.&lt;/li&gt;
&lt;li&gt;In-memory processing makes Spark faster than Hadoop MapReduce – up to 100 times for data in &lt;span class="caps"&gt;RAM&lt;/span&gt; and up to 10 times for data in&amp;nbsp;storage.&lt;/li&gt;
&lt;li&gt;Iterative&amp;nbsp;processing.&lt;/li&gt;
&lt;li&gt;If the task is to process data again and again – Spark defeats Hadoop MapReduce. Spark’s Resilient Distributed Datasets (RDDs) enable multiple map operations in memory, while Hadoop MapReduce has to write interim results to a&amp;nbsp;disk.&lt;/li&gt;
&lt;li&gt;Near real-time&amp;nbsp;processing.&lt;/li&gt;
&lt;li&gt;If a business needs immediate insights, then they should opt for Spark and its in-memory&amp;nbsp;processing.&lt;/li&gt;
&lt;li&gt;Graph&amp;nbsp;processing.&lt;/li&gt;
&lt;li&gt;Spark&amp;#8217;s computational model is good for iterative computations that are typical in graph processing. And Apache Spark has GraphX – an &lt;span class="caps"&gt;API&lt;/span&gt; for graph&amp;nbsp;computation.&lt;/li&gt;
&lt;li&gt;Machine&amp;nbsp;learning&lt;/li&gt;
&lt;li&gt;Spark has MLlib – a built-in machine learning library, while Hadoop needs a third-party to provide it. MLlib has out-of-the-box algorithms that also run in&amp;nbsp;memory.&lt;/li&gt;
&lt;li&gt;Joining&amp;nbsp;datasets&lt;/li&gt;
&lt;li&gt;Due to its speed, Spark can create all combinations faster, though Hadoop may be better if joining of very large data sets that requires a lot of shuffling and sorting is&amp;nbsp;needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="architecture-of-sparkupdating"&gt;Architecture of&amp;nbsp;Spark(Updating)&lt;/h1&gt;
&lt;h1 id="resilient-distributed-datasetsrdd"&gt;Resilient Distributed Datasets(&lt;span class="caps"&gt;RDD&lt;/span&gt;)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Dataset&lt;/li&gt;
&lt;li&gt;Distributed&lt;/li&gt;
&lt;li&gt;Resilient&lt;/li&gt;
&lt;li&gt;Recover from&amp;nbsp;failure&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">YiLi</dc:creator><pubDate>Wed, 21 Apr 2021 00:00:00 +0800</pubDate><guid isPermaLink="false">tag:jelly123456.github.io,2021-04-21:/2021/04/21/apache-spark/</guid><category>Big data&amp;Data Engineer</category></item><item><title>Data ETL</title><link>https://jelly123456.github.io/2021/04/21/data-etl/</link><description>&lt;h1 id="sources"&gt;Sources&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://alexioannides.com/category/data-science.html"&gt;Good blog from&amp;nbsp;internet&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="what-is-etl"&gt;What is &lt;span class="caps"&gt;ETL&lt;/span&gt;?&lt;/h1&gt;
&lt;p&gt;Extract, transform and&amp;nbsp;load&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">YiLi</dc:creator><pubDate>Wed, 21 Apr 2021 00:00:00 +0800</pubDate><guid isPermaLink="false">tag:jelly123456.github.io,2021-04-21:/2021/04/21/data-etl/</guid><category>Big data&amp;Data Engineer</category></item><item><title>pyspark - A python API for spark</title><link>https://jelly123456.github.io/2021/04/21/pyspark-a-python-api-for-spark/</link><description>&lt;p&gt;&lt;a href="https://medium.com/javarevisited/5-free-courses-to-learn-apache-spark-in-2020-bdff2d60c800"&gt;Source&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="steps-to-set-up-the-environment-in-windows"&gt;Steps to set up the environment in&amp;nbsp;Windows&lt;/h1&gt;
&lt;h2 id="1-setup-python"&gt;1. Setup&amp;nbsp;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Commands to check python is installed on&amp;nbsp;Windows&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;open a command&amp;nbsp;prompt&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;python&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="2-install-pycharm"&gt;2. Install&amp;nbsp;PyCharm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install&amp;nbsp;PyCharm&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="3-install-spark"&gt;3. Install&amp;nbsp;Spark&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install &lt;span class="caps"&gt;JDK&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dowanload&amp;nbsp;spark&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://spark.apache.org/downloads.html"&gt;Dowanload&amp;nbsp;Link&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup&amp;nbsp;Spark&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;set &amp;#8220;SPARK_HOME&amp;#8221; environment variable to the location where the &amp;#8220;spark&amp;#8221; is&amp;nbsp;installed&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;add &amp;#8220;%SPARK_HOME%\bin&amp;#8221; to &amp;#8220;path&amp;#8221;&amp;nbsp;variable&lt;/li&gt;
&lt;li&gt;open a terminal and type&amp;nbsp;&amp;#8220;pyspark&amp;#8221;.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install &amp;#8220;winutils.exe&amp;#8221; to aviod exception&amp;nbsp;error&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;download&amp;nbsp;&amp;#8220;winutils.exe&amp;#8221;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;save it to a folder where &amp;#8220;spark&amp;#8221; is installed&amp;nbsp;(SPARK_HOME\bin)&lt;/li&gt;
&lt;li&gt;set &amp;#8220;HADOOP_HOME&amp;#8221; to the location where &amp;#8220;spark&amp;#8221; is&amp;nbsp;installed&lt;/li&gt;
&lt;li&gt;add &amp;#8220;%HADOOP_HOME%\bin&amp;#8221; to &amp;#8220;path&amp;#8221;&amp;nbsp;variable&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add &lt;span class="caps"&gt;SPARK&lt;/span&gt; to pycharm &lt;span class="caps"&gt;IDE&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;open &amp;#8220;File&amp;#8221; =&amp;gt; &amp;#8220;Settings..&amp;#8221; =&amp;gt; Project:xxx =&amp;gt; Project structure =&amp;gt; &amp;#8220;Add Content&amp;nbsp;Root&amp;#8221;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt; folder&amp;nbsp;\python&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;SPARK&lt;/span&gt; folder&amp;nbsp;\python\py4j-0.10.9-src.zip&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="4-start-developing-spark-applications-with-pycharm"&gt;4. Start developing spark applications with&amp;nbsp;pycharm&lt;/h2&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">YiLi</dc:creator><pubDate>Wed, 21 Apr 2021 00:00:00 +0800</pubDate><guid isPermaLink="false">tag:jelly123456.github.io,2021-04-21:/2021/04/21/pyspark-a-python-api-for-spark/</guid><category>Big data&amp;Data Engineer</category></item><item><title>Scala and spark(Updating)</title><link>https://jelly123456.github.io/2021/04/21/scala-and-sparkupdating/</link><description>&lt;p&gt;&lt;a href="https://www.udemy.com/course/scala-and-spark-2-getting-started/learn/lecture/9656744?LSNPUBID=JVFxdTr9V80&amp;amp;components=purchase%2Ccacheable_buy_button%2Cbuy_button%2Crecommendation&amp;amp;ranEAID=JVFxdTr9V80&amp;amp;ranMID=39197&amp;amp;ranSiteID=JVFxdTr9V80-NEYgc584Fxj.ju4fAiJ35A&amp;amp;utm_medium=udemyads&amp;amp;utm_source=aff-campaign#overview"&gt;Source&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="steps-to-set-up-the-environment"&gt;Steps to set up the&amp;nbsp;environment&lt;/h1&gt;
&lt;h2 id="1-setup-java-and-jdk"&gt;1. Setup Java and &lt;span class="caps"&gt;JDK&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Commands to check &lt;span class="caps"&gt;JDK&lt;/span&gt; is installed on&amp;nbsp;Windows&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;open a command&amp;nbsp;prompt&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;javac -version&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="2-install-scala-with-intellij-ide"&gt;2. Install Scala with IntelliJ &lt;span class="caps"&gt;IDE&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install IntelliJ &lt;span class="caps"&gt;IDE&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Download and install&amp;nbsp;IntelliJ&lt;/li&gt;
&lt;li&gt;Install &lt;strong&gt;&lt;em&gt;scala&lt;/em&gt;&lt;/strong&gt;&amp;nbsp;plugins&lt;/li&gt;
&lt;li&gt;Updating&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">YiLi</dc:creator><pubDate>Wed, 21 Apr 2021 00:00:00 +0800</pubDate><guid isPermaLink="false">tag:jelly123456.github.io,2021-04-21:/2021/04/21/scala-and-sparkupdating/</guid><category>Big data&amp;Data Engineer</category></item><item><title>Hadoop and MapReduce</title><link>https://jelly123456.github.io/2021/04/19/hadoop-and-mapreduce/</link><description>&lt;p&gt;&lt;a href="https://medium.com/swlh/5-free-online-courses-to-learn-big-data-hadoop-and-spark-in-2019-a553e6ccfe30"&gt;Good blog from&amp;nbsp;internet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.udemy.com/course/hadoopstarterkit/learn/lecture/2995948#overview"&gt;Source&amp;nbsp;1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hadoop.apache.org/"&gt;Source&amp;nbsp;2&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="what-is-big-data"&gt;What is big&amp;nbsp;data&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Big&amp;nbsp;volume&lt;/li&gt;
&lt;li&gt;High&amp;nbsp;velocity&lt;/li&gt;
&lt;li&gt;Variety&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="problems-with-big-data"&gt;Problems with big&amp;nbsp;data&lt;/h1&gt;
&lt;p&gt;With traditional platform, a &lt;span class="caps"&gt;ETF&lt;/span&gt; task would take a very long time for big data because of low the data access rate and long program computation&amp;nbsp;time.&lt;/p&gt;
&lt;h1 id="hadoop"&gt;Hadoop&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;A big data&amp;nbsp;platform&lt;/li&gt;
&lt;li&gt;A open source project by Apache Software&amp;nbsp;Foundation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="hdfs-hadoop-distributed-file-system"&gt;&lt;span class="caps"&gt;HDFS&lt;/span&gt; - Hadoop Distributed File&amp;nbsp;System&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;If you upload a file into &lt;span class="caps"&gt;HDFS&lt;/span&gt;, it will automatically split into blocks with a fixed size of &lt;span class="caps"&gt;128MB&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;HDFS&lt;/span&gt;&amp;nbsp;responsibilities&lt;/li&gt;
&lt;li&gt;takes care of placing the blocks in differnet&amp;nbsp;nodes&lt;/li&gt;
&lt;li&gt;takes care of replicating each block to be more than one node, by feault, it will replicates to be 3&amp;nbsp;nodes&lt;/li&gt;
&lt;li&gt;Benefits of &lt;span class="caps"&gt;HDFS&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Support distributed processing&lt;ul&gt;
&lt;li&gt;Blocks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Handle failure&lt;ul&gt;
&lt;li&gt;Replicate&amp;nbsp;blocks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Scalability&lt;ul&gt;
&lt;li&gt;Able to support future&amp;nbsp;expansion&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cost effective&lt;ul&gt;
&lt;li&gt;Commodity&amp;nbsp;hardware&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="mapreduce"&gt;MapReduce&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;MapReduce is a programming model for distributed&amp;nbsp;computing&lt;/li&gt;
&lt;li&gt;It is not a programming language, but a&amp;nbsp;model&lt;/li&gt;
&lt;li&gt;It can process huge datasets in a distributed&amp;nbsp;fashion&lt;/li&gt;
&lt;li&gt;Phases&lt;/li&gt;
&lt;li&gt;Map phase&lt;ul&gt;
&lt;li&gt;Assign each individual&amp;nbsp;mapper&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Shuffle&amp;nbsp;phase&lt;/li&gt;
&lt;li&gt;Reduce phase&lt;ul&gt;
&lt;li&gt;Aggregrate the results from map&amp;nbsp;phase&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="apache-pig"&gt;Apache&amp;nbsp;Pig&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;Developed in&amp;nbsp;Yahoo&lt;/li&gt;
&lt;li&gt;To make mapreduce accessible to anyone who want to use hadoop&amp;nbsp;clusters&lt;/li&gt;
&lt;li&gt;The lines of codes will be less than that is written in&amp;nbsp;Java&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="apache-hive"&gt;Apache&amp;nbsp;Hive&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Description&lt;/li&gt;
&lt;li&gt;Developed by&amp;nbsp;Facebook&lt;/li&gt;
&lt;li&gt;Widely used in&amp;nbsp;industry&lt;/li&gt;
&lt;li&gt;Able to create table structures for your data set and write &lt;span class="caps"&gt;SQL&lt;/span&gt; quries to&amp;nbsp;analyze.&lt;/li&gt;
&lt;li&gt;Pig &lt;span class="caps"&gt;VS&lt;/span&gt;&amp;nbsp;Hive&lt;/li&gt;
&lt;li&gt;These two similar tools can be used in the same hadoop&amp;nbsp;project&lt;/li&gt;
&lt;li&gt;Pig is suitable to run standard nightly &lt;span class="caps"&gt;ETL&lt;/span&gt; jobs, like extracting data, transforming data and doing some pre-defined&amp;nbsp;calculations&lt;/li&gt;
&lt;li&gt;Hive can be used by developers, data analytics, data scientists on a daily ad-hoc data analysis&amp;nbsp;*&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">YiLi</dc:creator><pubDate>Mon, 19 Apr 2021 00:00:00 +0800</pubDate><guid isPermaLink="false">tag:jelly123456.github.io,2021-04-19:/2021/04/19/hadoop-and-mapreduce/</guid><category>Big data&amp;Data Engineer</category></item></channel></rss>